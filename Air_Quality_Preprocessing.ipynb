{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data_path = './data/'\n",
    "model_path = './model/'\n",
    "source_data_path = './data/MSBD5002PROJECT_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Air Quality Data (on hourly basis)  \n",
    "The Air Quality Data file contains the concentration of PM2.5 (ug/m3), PM10 (ug/m3), NO2 (ug/m3),\n",
    "CO (mg/m3), O3 (ug/m3) and SO2 (ug/m3) from Beijing. Your group only need to predict the\n",
    "concentration of PM2.5, PM10 and O3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1). airQuality_201701-201801.csv &emsp; January 2017 to January 2018  \n",
    "2). airQuality_201802-201803.csv &emsp; From February 2018 to March 2018  \n",
    "3). airQuality_201804.csv &emsp; &emsp; &emsp; &emsp; April 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read Data\n",
    "AQ_Jan = pd.read_csv(source_data_path+'airQuality_201701-201801.csv')\n",
    "AQ_Mar = pd.read_csv(source_data_path+'airQuality_201802-201803.csv')\n",
    "AQ_Apr = pd.read_csv(source_data_path+'airQuality_201804.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we combined the three datasets, check all columns' names are same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "del AQ_Apr.index.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AQ_Jan.rename(columns={'utc_time':'time'}, inplace=True)\n",
    "AQ_Mar.rename(columns={'utc_time':'time'}, inplace=True)\n",
    "AQ_Apr.rename(columns={'station_id':'stationId',\\\n",
    "                       'PM25_Concentration':'PM2.5','PM10_Concentration':'PM10',\\\n",
    "                       'NO2_Concentration':'NO2', 'CO_Concentration':'CO',\\\n",
    "                       'O3_Concentration':'O3','SO2_Concentration':'SO2'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['stationId', 'time', 'PM2.5', 'PM10', 'NO2', 'CO', 'O3', 'SO2'], dtype='object')\n",
      "Index(['stationId', 'time', 'PM2.5', 'PM10', 'NO2', 'CO', 'O3', 'SO2'], dtype='object')\n",
      "Index(['stationId', 'time', 'PM2.5', 'PM10', 'NO2', 'CO', 'O3', 'SO2'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(AQ_Jan.columns)\n",
    "print(AQ_Mar.columns)\n",
    "print(AQ_Apr.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-01-01 14:00:00\n",
      "2018-01-31 15:00:00\n",
      "2018-01-31 16:00:00\n",
      "2018-03-31 15:00:00\n",
      "2018-04-01 02:00:00\n",
      "2018-04-30 23:00:00\n"
     ]
    }
   ],
   "source": [
    "print(AQ_Jan.time.min())\n",
    "print(AQ_Jan.time.max())\n",
    "print(AQ_Mar.time.min())\n",
    "print(AQ_Mar.time.max())\n",
    "print(AQ_Apr.time.min())\n",
    "print(AQ_Apr.time.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "# All stations in AQ_Jan and AQ_Mar and AQ_Apr are the same. \n",
    "print(set(AQ_Jan.stationId.unique()) ^ set(AQ_Mar.stationId.unique()))\n",
    "print(set(AQ_Jan.stationId.unique()) ^ set(AQ_Apr.stationId.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 PreProcess AQ_Jan(Drop Duplicated rows)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessed air quality data firstly. There are 35 air quality stations who recorded pollutant concentrations from 2pm, Jan 1st, 2017 to 3pm, Jan 31th, 2018, with missed data. So we tried to find out missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 311010 entries, 0 to 311009\n",
      "Data columns (total 8 columns):\n",
      "stationId    311010 non-null object\n",
      "time         311010 non-null object\n",
      "PM2.5        290621 non-null float64\n",
      "PM10         227747 non-null float64\n",
      "NO2          292359 non-null float64\n",
      "CO           268197 non-null float64\n",
      "O3           290589 non-null float64\n",
      "SO2          292462 non-null float64\n",
      "dtypes: float64(6), object(2)\n",
      "memory usage: 19.0+ MB\n"
     ]
    }
   ],
   "source": [
    "AQ_Jan.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Stations: 35\n",
      "Count of TimeStamp(Hours): 8701\n"
     ]
    }
   ],
   "source": [
    "print('Count of Stations: {0}'.format(len(AQ_Jan.stationId.unique())))\n",
    "print('Count of TimeStamp(Hours): {0}'.format(len(AQ_Jan.time.unique())))\n",
    "# 35 x 8701 = 304535, But number of rows is 311010."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the number of duplicated rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6475"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = 0\n",
    "duplicated_column = pd.DataFrame(columns=AQ_Jan.columns.tolist())\n",
    "for i, value in enumerate(AQ_Jan.duplicated()):\n",
    "    if value:\n",
    "        duplicated_column.loc[counter] = AQ_Jan.loc[i]\n",
    "        counter += 1\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 304535 entries, 0 to 311009\n",
      "Data columns (total 8 columns):\n",
      "stationId    304535 non-null object\n",
      "time         304535 non-null object\n",
      "PM2.5        284771 non-null float64\n",
      "PM10         222667 non-null float64\n",
      "NO2          286480 non-null float64\n",
      "CO           262301 non-null float64\n",
      "O3           284701 non-null float64\n",
      "SO2          286575 non-null float64\n",
      "dtypes: float64(6), object(2)\n",
      "memory usage: 20.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# drop duplicated row in AQ_Jan\n",
    "AQ_Jan = AQ_Jan.drop_duplicates()\n",
    "AQ_Jan.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Combine all data together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality = pd.concat([AQ_Jan, AQ_Mar, AQ_Apr], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort \"air_quality\" by stationId and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality = air_quality.sort_values(by=['stationId','time'])\n",
    "air_quality = air_quality.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns 'CO','NO2','SO2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality = air_quality.drop(columns=['CO','NO2','SO2'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Find Outlier values(only consider PM2.5, PM10, O3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pollutants = air_quality.columns[2:].tolist()\n",
    "Stations = air_quality.stationId.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stat(df, attr):\n",
    "    max_ = df[[attr]].max()\n",
    "    min_ = df[[attr]].min()\n",
    "    median_ = df[[attr]].median()\n",
    "    mean_ = df[[attr]].mean()\n",
    "    return np.hstack((max_, min_, median_, mean_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The statistical information of PM2.5 is :\n",
      "\tMax: 1574.0\tMin: 2.0\tMedian: 40.0\tMean: 61.285504365847856\n",
      "The statistical information of PM10 is :\n",
      "\tMax: 3280.0\tMin: 5.0\tMedian: 73.0\tMean: 93.87728746397694\n",
      "The statistical information of O3 is :\n",
      "\tMax: 504.0\tMin: 1.0\tMedian: 48.0\tMean: 56.53468696164657\n"
     ]
    }
   ],
   "source": [
    "for pollutant in Pollutants:\n",
    "    pollutant_stat = get_stat(air_quality, pollutant)\n",
    "    print('The statistical information of {0} is :\\n\\tMax: {1}\\tMin: {2}\\tMedian: {3}\\tMean: {4}'\\\n",
    "          .format(pollutant, \\\n",
    "                  str(pollutant_stat[0]), str(pollutant_stat[1]), \n",
    "                  str(pollutant_stat[2]), str(pollutant_stat[3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pyecharts into Notebook\n",
    "1. Make sure you have nodeJs on your laptop.\n",
    "2. npm install -g phantomjs-prebuilt\n",
    "3. pip install pyecharts-snapshot\n",
    "4. pip install pyecharts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyecharts import Scatter\n",
    "scatter = Scatter(\"Scatter Plot: Find outlier values\",\n",
    "                  width=1200,height=600)\n",
    "for pollutant in Pollutants:\n",
    "    scatter.add(pollutant,\n",
    "                air_quality[pollutant].dropna().index,\n",
    "                air_quality[pollutant].dropna().values.flatten(),\n",
    "                is_datazoom_show=True,\n",
    "                datazoom_orient='vertical',\n",
    "                datazoom_range=[80,100],\n",
    "                legend_top='30',\n",
    "                mark_line=\"average\",\n",
    "                legend_selectedmode='single',\n",
    "                is_toolbox_show=False\n",
    "               )\n",
    "scatter.render('./image/Outliers.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers of PM2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stationId</th>\n",
       "      <th>time</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>O3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>139411</th>\n",
       "      <td>huairou_aq</td>\n",
       "      <td>2018-03-29 14:00:00</td>\n",
       "      <td>1574.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347640</th>\n",
       "      <td>yufa_aq</td>\n",
       "      <td>2017-05-05 02:00:00</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347641</th>\n",
       "      <td>yufa_aq</td>\n",
       "      <td>2017-05-05 03:00:00</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         stationId                 time   PM2.5  PM10    O3\n",
       "139411  huairou_aq  2018-03-29 14:00:00  1574.0   NaN  60.0\n",
       "347640     yufa_aq  2017-05-05 02:00:00  1004.0   NaN  99.0\n",
       "347641     yufa_aq  2017-05-05 03:00:00  1004.0   NaN  86.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_quality[air_quality['PM2.5']>1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# air_quality.iloc[139411,2]=np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers of O3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stationId</th>\n",
       "      <th>time</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>O3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14629</th>\n",
       "      <td>badaling_aq</td>\n",
       "      <td>2017-07-12 08:00:00</td>\n",
       "      <td>140.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>474.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15117</th>\n",
       "      <td>badaling_aq</td>\n",
       "      <td>2017-08-01 23:00:00</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33220</th>\n",
       "      <td>daxing_aq</td>\n",
       "      <td>2017-02-09 20:00:00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33316</th>\n",
       "      <td>daxing_aq</td>\n",
       "      <td>2017-02-13 20:00:00</td>\n",
       "      <td>134.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33412</th>\n",
       "      <td>daxing_aq</td>\n",
       "      <td>2017-02-17 20:00:00</td>\n",
       "      <td>140.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33457</th>\n",
       "      <td>daxing_aq</td>\n",
       "      <td>2017-02-19 20:00:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33590</th>\n",
       "      <td>daxing_aq</td>\n",
       "      <td>2017-02-25 20:00:00</td>\n",
       "      <td>19.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44639</th>\n",
       "      <td>dingling_aq</td>\n",
       "      <td>2017-03-09 12:00:00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44719</th>\n",
       "      <td>dingling_aq</td>\n",
       "      <td>2017-03-17 00:00:00</td>\n",
       "      <td>113.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44771</th>\n",
       "      <td>dingling_aq</td>\n",
       "      <td>2017-03-19 04:00:00</td>\n",
       "      <td>250.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44958</th>\n",
       "      <td>dingling_aq</td>\n",
       "      <td>2017-03-26 23:00:00</td>\n",
       "      <td>14.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44986</th>\n",
       "      <td>dingling_aq</td>\n",
       "      <td>2017-03-28 03:00:00</td>\n",
       "      <td>72.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45531</th>\n",
       "      <td>dingling_aq</td>\n",
       "      <td>2017-04-20 13:00:00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45837</th>\n",
       "      <td>dingling_aq</td>\n",
       "      <td>2017-05-05 11:00:00</td>\n",
       "      <td>19.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60804</th>\n",
       "      <td>donggaocun_aq</td>\n",
       "      <td>2017-11-17 14:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60805</th>\n",
       "      <td>donggaocun_aq</td>\n",
       "      <td>2017-11-17 15:00:00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60806</th>\n",
       "      <td>donggaocun_aq</td>\n",
       "      <td>2017-11-17 16:00:00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60807</th>\n",
       "      <td>donggaocun_aq</td>\n",
       "      <td>2017-11-17 17:00:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60808</th>\n",
       "      <td>donggaocun_aq</td>\n",
       "      <td>2017-11-17 18:00:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60809</th>\n",
       "      <td>donggaocun_aq</td>\n",
       "      <td>2017-11-17 19:00:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60810</th>\n",
       "      <td>donggaocun_aq</td>\n",
       "      <td>2017-11-17 20:00:00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60811</th>\n",
       "      <td>donggaocun_aq</td>\n",
       "      <td>2017-11-17 21:00:00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60812</th>\n",
       "      <td>donggaocun_aq</td>\n",
       "      <td>2017-11-17 22:00:00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60813</th>\n",
       "      <td>donggaocun_aq</td>\n",
       "      <td>2017-11-17 23:00:00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60814</th>\n",
       "      <td>donggaocun_aq</td>\n",
       "      <td>2017-11-18 00:00:00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60815</th>\n",
       "      <td>donggaocun_aq</td>\n",
       "      <td>2017-11-18 01:00:00</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64787</th>\n",
       "      <td>dongsi_aq</td>\n",
       "      <td>2017-01-06 07:00:00</td>\n",
       "      <td>245.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86597</th>\n",
       "      <td>fangshan_aq</td>\n",
       "      <td>2017-01-16 19:00:00</td>\n",
       "      <td>125.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>504.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253686</th>\n",
       "      <td>tiantan_aq</td>\n",
       "      <td>2017-09-30 23:00:00</td>\n",
       "      <td>168.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265502</th>\n",
       "      <td>tongzhou_aq</td>\n",
       "      <td>2017-11-13 07:00:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>495.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265503</th>\n",
       "      <td>tongzhou_aq</td>\n",
       "      <td>2017-11-13 08:00:00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            stationId                 time  PM2.5   PM10     O3\n",
       "14629     badaling_aq  2017-07-12 08:00:00  140.0  214.0  474.0\n",
       "15117     badaling_aq  2017-08-01 23:00:00   30.0    NaN  500.0\n",
       "33220       daxing_aq  2017-02-09 20:00:00    3.0   13.0  500.0\n",
       "33316       daxing_aq  2017-02-13 20:00:00  134.0  146.0  500.0\n",
       "33412       daxing_aq  2017-02-17 20:00:00  140.0  156.0  500.0\n",
       "33457       daxing_aq  2017-02-19 20:00:00    5.0   25.0  500.0\n",
       "33590       daxing_aq  2017-02-25 20:00:00   19.0   41.0  500.0\n",
       "44639     dingling_aq  2017-03-09 12:00:00    6.0   23.0  500.0\n",
       "44719     dingling_aq  2017-03-17 00:00:00  113.0  172.0  500.0\n",
       "44771     dingling_aq  2017-03-19 04:00:00  250.0    NaN  500.0\n",
       "44958     dingling_aq  2017-03-26 23:00:00   14.0   24.0  500.0\n",
       "44986     dingling_aq  2017-03-28 03:00:00   72.0  102.0  500.0\n",
       "45531     dingling_aq  2017-04-20 13:00:00    7.0   41.0  500.0\n",
       "45837     dingling_aq  2017-05-05 11:00:00   19.0  123.0  499.0\n",
       "60804   donggaocun_aq  2017-11-17 14:00:00    NaN    NaN  500.0\n",
       "60805   donggaocun_aq  2017-11-17 15:00:00    7.0   28.0  500.0\n",
       "60806   donggaocun_aq  2017-11-17 16:00:00   12.0   23.0  500.0\n",
       "60807   donggaocun_aq  2017-11-17 17:00:00    5.0   18.0  500.0\n",
       "60808   donggaocun_aq  2017-11-17 18:00:00    5.0   19.0  500.0\n",
       "60809   donggaocun_aq  2017-11-17 19:00:00   10.0   16.0  500.0\n",
       "60810   donggaocun_aq  2017-11-17 20:00:00    3.0   25.0  500.0\n",
       "60811   donggaocun_aq  2017-11-17 21:00:00   12.0   24.0  500.0\n",
       "60812   donggaocun_aq  2017-11-17 22:00:00    7.0   24.0  500.0\n",
       "60813   donggaocun_aq  2017-11-17 23:00:00    4.0   18.0  500.0\n",
       "60814   donggaocun_aq  2017-11-18 00:00:00    4.0   11.0  500.0\n",
       "60815   donggaocun_aq  2017-11-18 01:00:00    9.0   10.0  500.0\n",
       "64787       dongsi_aq  2017-01-06 07:00:00  245.0  256.0  500.0\n",
       "86597     fangshan_aq  2017-01-16 19:00:00  125.0    NaN  504.0\n",
       "253686     tiantan_aq  2017-09-30 23:00:00  168.0  181.0  500.0\n",
       "265502    tongzhou_aq  2017-11-13 07:00:00    5.0  124.0  495.0\n",
       "265503    tongzhou_aq  2017-11-13 08:00:00    8.0   89.0  500.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_quality[air_quality['O3']>=460]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# air_quality.iloc[253686,4]=np.nan\n",
    "# air_quality.iloc[86597, 4]=np.nan\n",
    "# air_quality.iloc[15117, 4]=np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Find missed data for each station. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-01-01 14:00:00\n",
      "2018-04-30 23:00:00\n"
     ]
    }
   ],
   "source": [
    "print(air_quality.time.min())\n",
    "print(air_quality.time.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "timeFrom = datetime.datetime.strptime('2017-01-01 14:00:00', \"%Y-%m-%d %H:%M:%S\")\n",
    "timeTo = datetime.datetime.strptime('2018-04-30 23:00:00', \"%Y-%m-%d %H:%M:%S\")\n",
    "Hours_Delta = pd.date_range(timeFrom, timeTo, freq='H').strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "Times_missed = {}\n",
    "\n",
    "for station in Stations:\n",
    "    Times_missed[station] = {}\n",
    "    # Find time not in rows.\n",
    "    tmp = air_quality[air_quality['stationId']==station].time.unique()\n",
    "    tmp = list(set(Hours_Delta) - set(tmp))\n",
    "    # In a Station, find time of row with missing pollutant concentation values.\n",
    "    for pollutant in Pollutants:\n",
    "        tmp_nan = air_quality[(air_quality['stationId']==station) & \\\n",
    "                              (air_quality[pollutant].isnull())].time.unique()     \n",
    "        Times_missed[station][pollutant] = np.concatenate((tmp,tmp_nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built a dataframe to record numbers of continuous missing hours for a station. \n",
    "from operator import itemgetter\n",
    "from itertools import groupby\n",
    "lost_hour_record = pd.DataFrame(columns=['stationId', 'pollutant', 'start_time', 'counts'])\n",
    "for station in Stations:\n",
    "    for pollutant in Pollutants:\n",
    "        tmp = [(i, int(datetime.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").timestamp()/3600)) \\\n",
    "               for i,x in enumerate(sorted(Times_missed[station][pollutant]))]\n",
    "        for key, group in groupby(tmp, lambda x: x[0]-x[1]):\n",
    "            group = list(group)\n",
    "            lose_hours = len(group)\n",
    "            start_hour = datetime.datetime.fromtimestamp(group[0][1]*3600).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            row_num = len(lose_hour_record.index)\n",
    "            lose_hour_record.loc[row_num] = [station, pollutant, start_hour, lose_hours]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lost_hour_record.to_csv(data_path+'lost_hour_record.csv',index=None)\n",
    "lost_hour_record = pd.read_csv(data_path+'lost_hour_record.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot missing data distribution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f40aca2a208>]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp = lost_hour_record[lost_hour_record['pollutant']=='PM2.5'].counts\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig, axs = plt.subplots(4, 1, sharex=True)\n",
    "\n",
    "# plot the same data on both axes\n",
    "for i in range(4):\n",
    "    axs[i].hist(tmp, bins=200)\n",
    "    \n",
    "# # zoom-in / limit the view to different portions of the data\n",
    "axs[0].set_ylim(bottom=1000)\n",
    "axs[1].set_ylim(bottom=100, top=1000)  \n",
    "axs[2].set_ylim(bottom=25, top=100)\n",
    "axs[3].set_ylim(bottom=0,top=20)\n",
    "axs[0].spines['bottom'].set_visible(False)\n",
    "axs[3].spines['top'].set_visible(False)\n",
    "axs[0].xaxis.set_visible(False)\n",
    "axs[1].xaxis.set_visible(False)\n",
    "axs[2].xaxis.set_visible(False)\n",
    "\n",
    "d = .005  # how big to make the diagonal lines in axes coordinates\n",
    "# arguments to pass to plot, just so we don't keep repeating them\n",
    "kwargs = dict(transform=axs[0].transAxes, color='k', clip_on=False)\n",
    "axs[0].plot((-d, +d), (-d, +d), **kwargs)        # top-left diagonal\n",
    "axs[0].plot((1 - d, 1 + d), (-d, +d), **kwargs)  # top-right diagonal\n",
    "\n",
    "for i in range(1,3):\n",
    "    axs[i].spines['bottom'].set_visible(False)\n",
    "    axs[i].spines['top'].set_visible(False)\n",
    "    \n",
    "    kwargs.update(transform=axs[i].transAxes)\n",
    "    axs[i].plot((-d, +d), (-d, +d), **kwargs)\n",
    "    axs[i].plot((1 - d, 1 + d), (-d, +d), **kwargs)\n",
    "    axs[i].plot((-d, +d), (1 - d, 1 + d), **kwargs)\n",
    "    axs[i].plot((1 - d, 1 + d), (1 - d, 1 + d), **kwargs)\n",
    "\n",
    "kwargs.update(transform=axs[3].transAxes)  # switch to the bottom axes\n",
    "axs[3].plot((-d, +d), (1 - d, 1 + d), **kwargs)  # bottom-left diagonal\n",
    "axs[3].plot((1 - d, 1 + d), (1 - d, 1 + d), **kwargs)  # bottom-right diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func1(pollutant='PM2.5'):\n",
    "    return lost_hour_record[lost_hour_record['pollutant']==pollutant].groupby(['counts']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyecharts import Bar\n",
    "bar = Bar(\"Distribution: Counts of continuous missed hours rows\",\n",
    "          'eg:PM2.5 1:6570 指PM2.5连续缺失1小时数据的情况出现6570次'\n",
    "         )\n",
    "for pollutant in Pollutants:\n",
    "    bar.add(pollutant,\n",
    "            func1(pollutant).index,\n",
    "            func1(pollutant).values, \n",
    "            legend_top='30',\n",
    "            legend_pos='right',\n",
    "            is_stack=True,\n",
    "            # default is X axis，horizontal\n",
    "            is_datazoom_show=True,\n",
    "            datazoom_type=\"slider\",\n",
    "            datazoom_range=[0, 50],\n",
    "            is_toolbox_show=False,\n",
    "            xaxis_name='Counts of continuous missed hours',\n",
    "            yaxis_name='Times',\n",
    "            yaxis_name_gap=60,\n",
    "            legend_selectedmode='single'\n",
    "           )\n",
    "bar.render('./image/lost_record.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Fill missing values(only consider PM2.5, PM10, O3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add rows with lost hours for each station.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add lose time data rows\n",
    "for station in Stations:\n",
    "    tmp = air_quality[air_quality['stationId']==station].time.unique()\n",
    "    tmp = list(set(Hours_Delta) - set(tmp))\n",
    "    tmp_df = pd.DataFrame(tmp, columns=['time'])\n",
    "    tmp_df['stationId'] = station\n",
    "    for pollutant in Pollutants:\n",
    "        tmp_df[pollutant] = np.nan\n",
    "    tmp_df = tmp_df[air_quality.columns]\n",
    "    air_quality = pd.concat([air_quality, tmp_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# air_quality = air_quality.sort_values(by=['stationId','time'])\n",
    "# air_quality = air_quality.reset_index(drop=True)\n",
    "# air_quality.to_csv(data_path+'air_quality.csv',index=None)\n",
    "air_quality = pd.read_csv(data_path+'air_quality.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "url: https://stackoverflow.com/questions/24870953/does-iterrows-have-performance-issues  \n",
    "Generally, iterrows should only be used in very very specific cases. This is the general order of precedence for performance of various operations:  \n",
    "1) vectorization  \n",
    "2) using a custom cython routine  \n",
    "3) apply:  a) reductions that can be performed in cython  b) iteration in python space  \n",
    "4) itertuples  \n",
    "5) iterrows  \n",
    "6) updating an empty frame (e.g. using loc one-row-at-a-time)  \n",
    "  \n",
    "Using a custom cython routine is usually too complicated, so let's skip that for now.    \n",
    "1) Vectorization is ALWAYS ALWAYS the first and best choice. However, there are a small set of cases which cannot be vectorized in obvious ways (mostly involving a recurrence). Further, on a smallish frame, it may be faster to do other methods.  \n",
    "3) Apply involves can usually be done by an iterator in Cython space (this is done internally in pandas) (this is a) case.  \n",
    "This is dependent on what is going on inside the apply expression. e.g. df.apply(lambda x: np.sum(x)) will be executed pretty swiftly (of course df.sum(1) is even better). However something like: df.apply(lambda x: x['b'] + 1) will be executed in python space, and consequently is slower.    \n",
    "4) itertuples does not box the data into a Series, just returns it as a tuple  \n",
    "5) iterrows DOES box the data into a Series. Unless you really need this, use another method  \n",
    "6) updating an empty frame a-single-row-at-a-time. I have seen this method used WAY too much. It is by far the slowest. It is probably common place (and reasonably fast for some python structures), but a DataFrame does a fair number of checks on indexing, so this will always be very slow to update a row at a time. Much better to create new structures and concat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For lost values within continuous hours of three, we fill them with the adjacent values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        None\n",
       "1        None\n",
       "2        None\n",
       "3        None\n",
       "4        None\n",
       "5        None\n",
       "6        None\n",
       "8        None\n",
       "9        None\n",
       "11       None\n",
       "12       None\n",
       "14       None\n",
       "16       None\n",
       "17       None\n",
       "18       None\n",
       "19       None\n",
       "20       None\n",
       "21       None\n",
       "22       None\n",
       "23       None\n",
       "25       None\n",
       "26       None\n",
       "29       None\n",
       "30       None\n",
       "31       None\n",
       "32       None\n",
       "33       None\n",
       "34       None\n",
       "35       None\n",
       "36       None\n",
       "         ... \n",
       "50231    None\n",
       "50232    None\n",
       "50233    None\n",
       "50234    None\n",
       "50235    None\n",
       "50236    None\n",
       "50237    None\n",
       "50238    None\n",
       "50239    None\n",
       "50240    None\n",
       "50241    None\n",
       "50242    None\n",
       "50243    None\n",
       "50245    None\n",
       "50246    None\n",
       "50247    None\n",
       "50248    None\n",
       "50249    None\n",
       "50250    None\n",
       "50251    None\n",
       "50252    None\n",
       "50253    None\n",
       "50254    None\n",
       "50255    None\n",
       "50256    None\n",
       "50257    None\n",
       "50258    None\n",
       "50259    None\n",
       "50260    None\n",
       "50261    None\n",
       "Length: 40631, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataFrame.interpolate(method='linear', axis=0, limit=None, inplace=False,\\\n",
    "#                       limit_direction='forward', limit_area=None, downcast=None, **kwargs)[source]\n",
    "\n",
    "# It took long time...\n",
    "# map pullutants and columns in air quality dataframe.\n",
    "pollutant_map = {'PM2.5': 2, 'PM10': 3, 'O3': 4}\n",
    "threshold=3\n",
    "def fill_nan_data(row):\n",
    "    \n",
    "        station_sindex = air_quality[(air_quality['stationId']==row['stationId']) & \\\n",
    "                                     (air_quality['time']=='2017-01-01 14:00:00')].index[0]\n",
    "        station_eindex = air_quality[(air_quality['stationId']==row['stationId']) & \\\n",
    "                                     (air_quality['time']=='2018-04-30 23:00:00')].index[0]\n",
    "        start_index = air_quality[(air_quality['stationId']==row['stationId']) & \\\n",
    "                                  (air_quality['time']==row['start_time'])].index[0]-1\n",
    "        end_index = start_index + 1 + row['counts']\n",
    "        \n",
    "        if start_index >= station_sindex and end_index <= station_eindex:\n",
    "            p_index = pollutant_map[row['pollutant']]\n",
    "            interval = air_quality.iloc[end_index, p_index] - air_quality.iloc[start_index, p_index]\n",
    "            s_concentration = air_quality.iloc[start_index, p_index]\n",
    "            time_counts = row['counts'] + 1\n",
    "            for i in range(start_index+1, end_index):\n",
    "                if interval == 0:\n",
    "                    air_quality.iloc[i, p_index] = s_concentration\n",
    "                else:\n",
    "                    air_quality.iloc[i, p_index] = s_concentration + interval/time_counts*(i-start_index)\n",
    "\n",
    "lost_hour_record[lost_hour_record.counts<=threshold].apply(lambda row: fill_nan_data(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# air_quality.to_csv(data_path+'air_quality_fillthree.csv')\n",
    "air_quality = pd.read_csv(data_path+'air_quality_fillthree.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the rest of lossing values, we consider pretrain a model of a pollutant to fill them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = pd.read_csv(data_path+'air_quality_station_info.csv',index_col=0)\n",
    "time_info = pd.read_csv(data_path+'time_info.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_new = air_quality.columns.append(time_info.columns[2:])\n",
    "air_quality = air_quality.merge(time_info,on='time',how='left')[columns_new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality.to_csv(data_path+'air_quality_fillthree.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use last three days(72 hours) datas as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>O3</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>weekday</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PM2.5</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.742676</td>\n",
       "      <td>-0.114345</td>\n",
       "      <td>0.012630</td>\n",
       "      <td>-0.140430</td>\n",
       "      <td>-0.047541</td>\n",
       "      <td>0.003905</td>\n",
       "      <td>0.019917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PM10</th>\n",
       "      <td>0.742676</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.014273</td>\n",
       "      <td>0.064326</td>\n",
       "      <td>-0.097578</td>\n",
       "      <td>-0.015307</td>\n",
       "      <td>0.017391</td>\n",
       "      <td>0.007086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O3</th>\n",
       "      <td>-0.114345</td>\n",
       "      <td>-0.014273</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.023517</td>\n",
       "      <td>-0.041284</td>\n",
       "      <td>0.005326</td>\n",
       "      <td>-0.014551</td>\n",
       "      <td>-0.250273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>0.012630</td>\n",
       "      <td>0.064326</td>\n",
       "      <td>-0.023517</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.497294</td>\n",
       "      <td>-0.010794</td>\n",
       "      <td>-0.006125</td>\n",
       "      <td>-0.000499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>-0.140430</td>\n",
       "      <td>-0.097578</td>\n",
       "      <td>-0.041284</td>\n",
       "      <td>-0.497294</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010925</td>\n",
       "      <td>0.018513</td>\n",
       "      <td>-0.001126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>-0.047541</td>\n",
       "      <td>-0.015307</td>\n",
       "      <td>0.005326</td>\n",
       "      <td>-0.010794</td>\n",
       "      <td>0.010925</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.007794</td>\n",
       "      <td>-0.001458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekday</th>\n",
       "      <td>0.003905</td>\n",
       "      <td>0.017391</td>\n",
       "      <td>-0.014551</td>\n",
       "      <td>-0.006125</td>\n",
       "      <td>0.018513</td>\n",
       "      <td>-0.007794</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour</th>\n",
       "      <td>0.019917</td>\n",
       "      <td>0.007086</td>\n",
       "      <td>-0.250273</td>\n",
       "      <td>-0.000499</td>\n",
       "      <td>-0.001126</td>\n",
       "      <td>-0.001458</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            PM2.5      PM10        O3      year     month       day   weekday  \\\n",
       "PM2.5    1.000000  0.742676 -0.114345  0.012630 -0.140430 -0.047541  0.003905   \n",
       "PM10     0.742676  1.000000 -0.014273  0.064326 -0.097578 -0.015307  0.017391   \n",
       "O3      -0.114345 -0.014273  1.000000 -0.023517 -0.041284  0.005326 -0.014551   \n",
       "year     0.012630  0.064326 -0.023517  1.000000 -0.497294 -0.010794 -0.006125   \n",
       "month   -0.140430 -0.097578 -0.041284 -0.497294  1.000000  0.010925  0.018513   \n",
       "day     -0.047541 -0.015307  0.005326 -0.010794  0.010925  1.000000 -0.007794   \n",
       "weekday  0.003905  0.017391 -0.014551 -0.006125  0.018513 -0.007794  1.000000   \n",
       "hour     0.019917  0.007086 -0.250273 -0.000499 -0.001126 -0.001458  0.001304   \n",
       "\n",
       "             hour  \n",
       "PM2.5    0.019917  \n",
       "PM10     0.007086  \n",
       "O3      -0.250273  \n",
       "year    -0.000499  \n",
       "month   -0.001126  \n",
       "day     -0.001458  \n",
       "weekday  0.001304  \n",
       "hour     1.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_quality.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_data(air_quality, length=24*3):\n",
    "    attr = ['PM2.5','PM10','O3','weekday','hour']\n",
    "    train_data = []\n",
    "    for station in Stations:\n",
    "        station_data = air_quality[air_quality.stationId==station].sort_values(by='time')\n",
    "        station_data = station_data[attr].values\n",
    "        for i in range(0, station_data.shape[0]-length):\n",
    "            # Prepare a line of training dataset.\n",
    "            # Features = stationId, stationType, Weekday, Hour\n",
    "            row = [station, stations[stations.stationId==station].station_type.values[0],\\\n",
    "                   station_data[i+length,-2], station_data[i+length,-1]]\n",
    "            history_data = station_data[i:i+length,:3].T.flatten().tolist()\n",
    "            target_data = station_data[i+length,:3].flatten().tolist()\n",
    "            row += history_data + target_data\n",
    "            if pd.isna(row).sum() == 0:\n",
    "                train_data.append(row)\n",
    "    return train_data\n",
    "train_data = build_train_data(air_quality, 72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131692"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using numpy.save_file met an error. So use another function from package of csv.b\n",
    "import csv\n",
    "\n",
    "with open(data_path+\"air_quality_pretrain_data.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statfeatures(narray, pollutant='PM2.5', d='H'):\n",
    "    '''get_statfeatures(narray, d='H')\n",
    "    Since features of three pollutants are the same, which is not suitable to train three models, \n",
    "    we added another statistical feature for perticular pollutant.\n",
    "    narray: air_quality[:, 4:4+72], namely history data of a perticular pollutant.\n",
    "    '''\n",
    "    if d=='H':\n",
    "        # compute last 72 hours data. (Return 5 features)\n",
    "        max_ = np.max(narray, axis=1)\n",
    "        min_ = np.min(narray, axis=1)\n",
    "        mean_ = np.mean(narray, axis=1)\n",
    "        median_ = np.median(narray, axis=1)\n",
    "        var_ = np.var(narray, axis=1)\n",
    "        return np.hstack((mean_, median_, max_, min_, var_)).reshape(-1,5)\n",
    "    elif d=='D':\n",
    "        # In last 3 day, compute every day's data. (Return 3*5 features)\n",
    "        stat_outcome = np.array([[] for i in range(narray.shape[0])])\n",
    "        for i in range(int(narray.shape[1]/24)):\n",
    "            stat_day = get_statfeatures(narray[:,i*24:(i+1)*24], d='H')\n",
    "            stat_outcome = np.hstack((stat_outcome, stat_day))\n",
    "        return stat_outcome\n",
    "\n",
    "\n",
    "def get_onehot(narray, stationType_map, stationId_map):\n",
    "    '''get_onehot(narray)\n",
    "    Format four features: stationId(35), stationType(4), weekday(7), hour(24) into seventy bool-typed features.\n",
    "    narray: air_quality[:,:4], namely all four features of training data \n",
    "    '''\n",
    "    onehot_data = []\n",
    "    for i in range(narray.shape[0]):\n",
    "        onehot_row = np.zeros(70)\n",
    "        # stationId\n",
    "        onehot_row[stationId_map[narray[i,0]]] = 1\n",
    "        # stationType\n",
    "        onehot_row[35+stationType_map[narray[i,1]]] = 1\n",
    "        # weekday\n",
    "        onehot_row[39+int(narray[i,2])] = 1\n",
    "        # hour\n",
    "        onehot_row[46+int(narray[i,3])] = 1\n",
    "        \n",
    "        onehot_data.append(onehot_row)\n",
    "    return np.array(onehot_data)\n",
    "\n",
    "\n",
    "def build_features(train_data, stations, pollutant='PM2.5', length=24*3):\n",
    "    '''\n",
    "    train_data: source pre-train data.\n",
    "    stations: information of each station, a dataframe.\n",
    "    '''\n",
    "    if pollutant == 'PM2.5':\n",
    "        static_hour = get_statfeatures(train_data[:,4:4+length], d='H')\n",
    "        static_days = get_statfeatures(train_data[:,4:4+length], d='D')\n",
    "    elif pollutant == 'PM10':\n",
    "        static_hour = get_statfeatures(train_data[:, 4+length:4+length*2], d='H')\n",
    "        static_days = get_statfeatures(train_data[:,4+length:4+length*2], d='D')\n",
    "    else:\n",
    "        static_hour = get_statfeatures(train_data[:, 4+length*2:4+length*3], d='H')\n",
    "        static_days = get_statfeatures(train_data[:,4+length*2:4+length*3], d='D')\n",
    "        \n",
    "    stationId_map = {}\n",
    "    for index, station in enumerate(sorted(stations.stationId.unique().tolist())):\n",
    "        stationId_map[station] = index\n",
    "    stationType_map = {}\n",
    "    for index, stationtype in enumerate(stations.station_type.unique().tolist()):\n",
    "        stationType_map[stationtype] = index\n",
    "        \n",
    "    onehot = get_onehot(train_data[:,:4], stationType_map, stationId_map)\n",
    "    \n",
    "    return np.hstack((onehot, train_data[:,4:-3], static_hour, static_days))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def built_model(train_data, stations, pollutant='PM2.5', length=24*3):\n",
    "    if pollutant == 'PM2.5':\n",
    "        dataset_Y = train_data[:,-3]\n",
    "    elif pollutant == 'PM10':\n",
    "        dataset_Y = train_data[:,-2]\n",
    "    else:\n",
    "        dataset_Y = train_data[:,-1]   \n",
    "    dataset_X = build_features(train_data, stations, pollutant, length)\n",
    "    train_X, test_X, train_Y, test_Y = train_test_split(dataset_X, dataset_Y, test_size=0.2, random_state=10)\n",
    "    \n",
    "    # load regression model\n",
    "    reg = xgb.XGBRegressor()\n",
    "    reg.fit(train_X, train_Y)\n",
    "    \n",
    "    # validation\n",
    "    pred_Y = reg.predict(test_X)\n",
    "    validation_error = np.mean(np.abs(pred_Y - test_Y) / (pred_Y + test_Y) * 2)\n",
    "    print('Validation error of {0} is {1}'.format(pollutant, str(validation_error)))\n",
    "    \n",
    "    model_file = model_path + pollutant + '_fillmissingdata.model'\n",
    "    joblib.dump(reg, model_file)\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell on Colab.\n",
    "\n",
    "reg_PM25 = built_model(train_data, stations, 'PM2.5')\n",
    "# Validation error of PM2.5 is 0.21988583652019708\n",
    "\n",
    "reg_PM10 = built_model(train_data, stations, 'PM10')\n",
    "# Validation error of PM10 is 0.19306589695892443\n",
    "\n",
    "reg_O3 = built_model(train_data, stations, 'O3')\n",
    "# Validation error of O3 is 0.2787606643604839"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use trained model to fill missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
